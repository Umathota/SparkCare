{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9e1993",
   "metadata": {},
   "source": [
    "# Data Analysis (Spark)\n",
    "Once we have made the data ready for analysis, we have to perform below analysis queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d01ce39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|         airlines|\n",
      "|            cog41|\n",
      "|          default|\n",
      "|healthcare_system|\n",
      "|          sumitdb|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all the Databases of Hive\n",
    "df = spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de02d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USe healthcare_system Database\n",
    "spark.sql(\"use healthcare_system\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26758fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------+\n",
      "|         database| tableName|isTemporary|\n",
      "+-----------------+----------+-----------+\n",
      "|healthcare_system|    claims|      false|\n",
      "|healthcare_system|   disease|      false|\n",
      "|healthcare_system|    groups|      false|\n",
      "|healthcare_system|grp_subgrp|      false|\n",
      "|healthcare_system|  hospital|      false|\n",
      "|healthcare_system|   patient|      false|\n",
      "|healthcare_system|  subgroup|      false|\n",
      "|healthcare_system|subscriber|      false|\n",
      "+-----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all the tables which are present in the healthcare_system database.\n",
    "sparkdf = spark.sql(\"show tables\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/Database.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dd0a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|         claim_id|      int|   null|\n",
      "|       patient_id|      int|   null|\n",
      "|     disease_name|   string|   null|\n",
      "|           sub_id|   string|   null|\n",
      "|claim_or_rejected|   string|   null|\n",
      "|       claim_type|   string|   null|\n",
      "|     claim_amount|   double|   null|\n",
      "|       claim_date|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the claims table\n",
    "sparkdf = spark.sql(\"desc claims\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/Claims_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bfca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|  disease_id|      int|   null|\n",
      "|disease_name|   string|   null|\n",
      "|   subgrp_id|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the disease table\n",
    "sparkdf = spark.sql(\"desc disease\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/disease_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab450a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|         grp_sk|      int|   null|\n",
      "|         grp_id|   string|   null|\n",
      "|       grp_name|   string|   null|\n",
      "|premium_written|      int|   null|\n",
      "|           city|   string|   null|\n",
      "|       zip_code|      int|   null|\n",
      "|        country|   string|   null|\n",
      "|       grp_type|   string|   null|\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the groups table\n",
    "sparkdf = spark.sql(\"desc groups\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/groups_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c95cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|grpsub_sk|      int|   null|\n",
      "|     g_id|   string|   null|\n",
      "|     s_id|   string|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the grp_subgrp table\n",
    "sparkdf = spark.sql(\"desc grp_subgrp\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/grp_subgrp_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f1cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|  hospital_id|   string|   null|\n",
      "|hospital_name|   string|   null|\n",
      "|         city|   string|   null|\n",
      "|        state|   string|   null|\n",
      "|      country|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the hospital table\n",
    "sparkdf = spark.sql(\"desc hospital\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/hospital_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb94bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-------+\n",
      "|          col_name|data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|        patient_id|      int|   null|\n",
      "|      patient_name|   string|   null|\n",
      "|    patient_gender|   string|   null|\n",
      "|patient_birth_date|   string|   null|\n",
      "|     patient_phone|   string|   null|\n",
      "|      disease_name|   string|   null|\n",
      "|              city|   string|   null|\n",
      "|       hospital_id|   string|   null|\n",
      "+------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the patient table\n",
    "sparkdf = spark.sql(\"desc patient\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/patient_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11071fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|      subgrp_sk|      int|   null|\n",
      "|      subgrp_id|   string|   null|\n",
      "|    subgrp_name|   string|   null|\n",
      "|monthly_premium|   double|   null|\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subgroup table\n",
    "sparkdf = spark.sql(\"desc subgroup\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/subgroup_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "487012d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|    sub_id|   string|   null|\n",
      "|first_name|   string|   null|\n",
      "| last_name|   string|   null|\n",
      "|    street|   string|   null|\n",
      "|birth_date|   string|   null|\n",
      "|    gender|   string|   null|\n",
      "|     phone|   string|   null|\n",
      "|      city|   string|   null|\n",
      "|  zip_code|      int|   null|\n",
      "|   country|   string|   null|\n",
      "| subgrp_id|   string|   null|\n",
      "|  elig_ind|   string|   null|\n",
      "|  eff_date|   string|   null|\n",
      "| term_date|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subscriber table\n",
    "sparkdf = spark.sql(\"desc subscriber\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/subscriber_tble.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba437b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which disease having maximum number of claims.\n",
    "results = spark.sql(\"select disease_name,count(claim_id) as \\\n",
    "                    max from claims group by disease_name order by max desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d92f2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+\n",
      "|    disease_name|max|\n",
      "+----------------+---+\n",
      "|        Glaucoma|  3|\n",
      "| Phenylketonuria|  3|\n",
      "|    Galactosemia|  3|\n",
      "|     Pet allergy|  3|\n",
      "|         Anthrax|  3|\n",
      "|    Head banging|  3|\n",
      "|          Asthma|  2|\n",
      "|          Scurvy|  2|\n",
      "|Drug consumption|  2|\n",
      "|  Bladder cancer|  2|\n",
      "|   Rett Syndrome|  2|\n",
      "|         Choking|  2|\n",
      "|             Flu|  2|\n",
      "|      Lymphedema|  2|\n",
      "|    Mold allergy|  2|\n",
      "| Fanconi anaemia|  2|\n",
      "|         Measles|  2|\n",
      "|          Stroke|  2|\n",
      "|         Cholera|  2|\n",
      "|         Malaria|  2|\n",
      "+----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:===================================================>  (190 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.toPandas().to_csv('Spark Outputs for Visualization/query1.csv')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3754f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find those Subscribers having age less than 30 and they subscribe any subgroup\n",
    "subscriber_ages = spark.sql(\"select birth_date,current_date() as CurrentDate,\\\n",
    "          year(current_date())-year(birth_date) as age from subscriber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d61882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = subscriber_ages.filter(subscriber_ages.age < 30).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "563e43b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribers having age less than 30 -->  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Subscribers having age less than 30 --> \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3f2f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  g_id|max|\n",
      "+------+---+\n",
      "|GRP104|  2|\n",
      "|GRP147|  2|\n",
      "|GRP143|  2|\n",
      "|GRP108|  1|\n",
      "|GRP105|  1|\n",
      "|GRP101|  1|\n",
      "|GRP148|  1|\n",
      "|GRP102|  1|\n",
      "|GRP103|  1|\n",
      "|GRP114|  1|\n",
      "|GRP142|  1|\n",
      "|GRP133|  1|\n",
      "|GRP112|  1|\n",
      "|GRP122|  1|\n",
      "|GRP138|  1|\n",
      "|GRP157|  1|\n",
      "|GRP127|  1|\n",
      "|GRP123|  1|\n",
      "|GRP126|  1|\n",
      "|GRP110|  1|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which group has maximum subgroups.\n",
    "sparkdf = spark.sql(\"select g_id,count(s_id) as max from grp_subgrp group by g_id order by max desc\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query3.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da47fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|       hospital_name|total_patient|\n",
      "+--------------------+-------------+\n",
      "|   Manipal Hospitals|            9|\n",
      "|Apollo Hospitals ...|            8|\n",
      "|Medanta The Medicity|            7|\n",
      "|Jaslok Hospital a...|            6|\n",
      "|Indraprastha Apol...|            5|\n",
      "|PGIMER - Postgrad...|            4|\n",
      "|Apollo Hospital -...|            4|\n",
      "|Fortis Hospital M...|            4|\n",
      "|King Edward Memor...|            3|\n",
      "|Apollo Health Cit...|            3|\n",
      "|Yashoda Hospital ...|            3|\n",
      "|Bombay Hospital &...|            3|\n",
      "|Fortis Hiranandan...|            2|\n",
      "|Lilavati Hospital...|            2|\n",
      "|The Christian Med...|            2|\n",
      "|Fortis Flt. Lt. R...|            1|\n",
      "|P. D. Hinduja Nat...|            1|\n",
      "|Breach Candy Hosp...|            1|\n",
      "|All India Institu...|            1|\n",
      "|Sir Ganga Ram Hos...|            1|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:==============================================>       (174 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find out hospital which serve most number of patients\n",
    "sparkdf = spark.sql(\"select hospital_name,count(patient_id) as total_patient \\\n",
    "          from hospital join patient on hospital.hospital_id=patient.hospital_id \\\n",
    "          group by hospital_name order by total_patient desc\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query4.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "598f14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|        subgrp_name|cunt|\n",
      "+-------------------+----+\n",
      "|            Therapy|  14|\n",
      "|         Hereditary|  11|\n",
      "|              Viral|  11|\n",
      "|Deficiency Diseases|  11|\n",
      "|         Physiology|  10|\n",
      "|          Allergies|  10|\n",
      "|           Accident|  10|\n",
      "|             Cancer|   9|\n",
      "|     Self inflicted|   7|\n",
      "| Infectious disease|   7|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which subgroups subscribe most number of times\n",
    "sparkdf = spark.sql(\"select subgrp_name, \\\n",
    "          count(sub_id) as cunt from subgroup join \\\n",
    "          subscriber on subgroup.subgrp_id = subscriber.subgrp_id  group by subgrp_name order by cunt desc\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query5.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38ca900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|claim_or_rejected|count(claim_id)|\n",
      "+-----------------+---------------+\n",
      "|                Y|             18|\n",
      "|                N|             52|\n",
      "+-----------------+---------------+\n",
      "\n",
      "The above Result shows the total 52 claims were rejected\n"
     ]
    }
   ],
   "source": [
    "# Find out total number of claims which were rejected\n",
    "sparkdf = spark.sql(\"select claim_or_rejected,count(claim_id) from claims group by claim_or_rejected\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query6.csv')\n",
    "sparkdf.show()\n",
    "print(\"The above Result shows the total 52 claims were rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7511581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|        city|maxclaim|\n",
      "+------------+--------+\n",
      "|      Mysore|       2|\n",
      "|    Amravati|       2|\n",
      "|   Kamarhati|       2|\n",
      "|    Jabalpur|       2|\n",
      "|Bihar Sharif|       2|\n",
      "|   Ghaziabad|       2|\n",
      "|       Morbi|       2|\n",
      "|  Karimnagar|       2|\n",
      "|   Bangalore|       1|\n",
      "|     Udaipur|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From where most claims are comming (city)\n",
    "sparkdf = spark.sql(\"select patient.city,count(claim_id) as maxclaim from patient join claims on \\\n",
    "          claims.patient_id = patient.patient_id group by patient.city order by maxclaim desc limit 10\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query7.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c816111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|grp_type|count(grp_id)|\n",
      "+--------+-------------+\n",
      "|   Govt.|            7|\n",
      "| Private|           51|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which groups of policies subscriber subscribe mostly Goverment or private\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_id) from groups group by grp_type\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query8.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96460808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|grp_type|count(g_id)|\n",
      "+--------+-----------+\n",
      "|   Govt.|         35|\n",
      "| Private|        347|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"select grp_subgrp.g_id from subscriber \\\n",
    "          join grp_subgrp on grp_subgrp.s_id = subscriber.subgrp_id\")\n",
    "\n",
    "res.registerTempTable(\"grp_tble\")\n",
    "\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_tble.g_id) from groups \\\n",
    "          join grp_tble on groups.grp_id = grp_tble.g_id group by grp_type\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query9.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28c891c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Above Result shows that most of the subscribers subscribe private groups\n"
     ]
    }
   ],
   "source": [
    "print(\"The Above Result shows that most of the subscribers subscribe private groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e9dca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|subgrp_id|avg(monthly_premium)|\n",
      "+---------+--------------------+\n",
      "|     S101|              3000.0|\n",
      "|     S102|              1000.0|\n",
      "|     S103|              2000.0|\n",
      "|     S104|              1500.0|\n",
      "|     S105|              2300.0|\n",
      "|     S106|              1200.0|\n",
      "|     S107|              3200.0|\n",
      "|     S108|              1500.0|\n",
      "|     S109|              2000.0|\n",
      "|     S110|              1000.0|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average monthly premium subscriber pay to insurance company subgroup.\n",
    "sparkdf = spark.sql(\"select subgroup.subgrp_id,avg(monthly_premium) from subscriber right join \\\n",
    "          subgroup on subscriber.subgrp_id = subgroup.subgrp_id group by \\\n",
    "          subgroup.subgrp_id order by subgroup.subgrp_id\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query10.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7771fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|grp_id|premium_written|\n",
      "+------+---------------+\n",
      "|GRP147|          99000|\n",
      "|GRP131|          99000|\n",
      "|GRP123|          99000|\n",
      "|GRP118|          97000|\n",
      "|GRP154|          95000|\n",
      "|GRP133|          93000|\n",
      "|GRP157|          92000|\n",
      "|GRP134|          90000|\n",
      "|GRP143|          90000|\n",
      "|GRP106|          89000|\n",
      "|GRP129|          88000|\n",
      "|GRP152|          87000|\n",
      "|GRP153|          86000|\n",
      "|GRP150|          84000|\n",
      "|GRP124|          81000|\n",
      "|GRP122|          79000|\n",
      "|GRP115|          79000|\n",
      "|GRP121|          78000|\n",
      "|GRP109|          78000|\n",
      "|GRP101|          72000|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out Which group is most profitable\n",
    "sparkdf = spark.sql(\"select grp_id,premium_written from groups order by premium_written desc\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query11.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e20b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+\n",
      "|patient_id|patient_name|age|\n",
      "+----------+------------+---+\n",
      "|    194166|          NA|  9|\n",
      "|    197441|    Deependu| 18|\n",
      "+----------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all the patients below age of 18 who admit for cancer\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_id, patient_name,\\\n",
    "          year(current_date())-year(patient_birth_date) as age from patient \\\n",
    "          where disease_name like '%cancer' order by age limit 2\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query12.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fee0245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+\n",
      "|patient_name|patient_gender|patient_birth_date|\n",
      "+------------+--------------+------------------+\n",
      "|   Anjushree|          Male|        1982-06-28|\n",
      "|  Chitranjan|        Female|        2020-10-27|\n",
      "|      Gensho|          Male|        1991-07-27|\n",
      "|  Vaijayanti|          Male|        1969-04-06|\n",
      "|       Aakar|        Female|        1990-04-24|\n",
      "|          NA|        Female|        1959-01-06|\n",
      "|       Saroj|        Female|        1953-07-21|\n",
      "|     Bhagvan|        Female|        2011-02-26|\n",
      "|  Dharmadaas|          Male|        1964-10-25|\n",
      "|       Umang|        Female|        2017-02-26|\n",
      "|          NA|          Male|        1955-06-03|\n",
      "|      Kishan|          Male|        1955-06-30|\n",
      "|          NA|        Female|        1953-04-04|\n",
      "|     Devnath|        Female|        1982-02-22|\n",
      "|      Harbir|        Female|        1960-02-24|\n",
      "|       Ekant|          Male|        1969-11-01|\n",
      "|          NA|          Male|        2013-10-30|\n",
      "|          NA|          Male|        1956-04-04|\n",
      "|       Lalit|        Female|        1978-04-30|\n",
      "|     Ujjawal|          Male|        1965-12-31|\n",
      "+------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List patients who have cashless insura0nce and have total charges greater than or equal for Rs. 50,000.\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_name,patient_gender,patient_birth_date \\\n",
    "          from patient join claims on patient.patient_id = claims.patient_id \\\n",
    "          where claim_amount >= 50000 and claim_type = 'claims of value'\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query13.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7eeaeb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|patient_name|\n",
      "+------------+\n",
      "|     Upasana|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List female patients over the age of 40 that have undergone knee surgery in the past year\n",
    "\n",
    "sparkdf = spark.sql(\"select patient_name from patient where patient_gender = 'Female' and disease_name = 'Heart Attack'\")\n",
    "sparkdf.toPandas().to_csv('Spark Outputs for Visualization/query14.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cee7f",
   "metadata": {},
   "source": [
    "# Thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
